{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1VIKq3q6c/fPofNp9Ble7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blueberrylazjy/kadai/blob/main/optimizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "最適化アルゴリズム(optimization algorithm)"
      ],
      "metadata": {
        "id": "V4QOKQfG4d-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "bH29AWfq3hYf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "最適化アルゴリズムは「勾配」を使用する\\\n",
        "$\\quad$ $\\omega$を多数のパラメータのうちの１つ\\\n",
        "$\\quad$ $E$誤差とする\n",
        "\n",
        "$$ \\frac{\\partial{E}}{\\partial{\\omega}}$$"
      ],
      "metadata": {
        "id": "v6fk4SBn4qv8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "勾配降下法(gradient descent)"
      ],
      "metadata": {
        "id": "SWvdBEew5O1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.確率的勾配降下法「SGD」 (stochastic gradient descent)\n",
        "\n",
        "$\\omega$　あるパラメータ\\\n",
        "$E \\quad$ 誤差\\\n",
        "$\\eta$ 「学習係数」(learning rate)\n",
        "\n",
        "$$ \\omega ← \\omega - \\eta\\frac{\\partial{E}}{\\partial{\\omega}}$$"
      ],
      "metadata": {
        "id": "-IxuXnZj5Z-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "optimizer = optim.SGD(..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "id": "JlDfkT4j4iR4",
        "outputId": "3f4c4651-c43a-4f0c-b08f-c8f2e3eb31dc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-b96b1de4fa6d>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    optimizer = optim.SGD(...\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "「慣性」(momentum)\n",
        "\n",
        "$Δ\\omega$　前回の更新量\\\n",
        "$\\alpha \\quad$ 慣性の強さを決める定数\n",
        "\n",
        "$$ \\omega ← \\omega - \\eta\\frac{\\partial{E}}{\\partial{\\omega}} + αΔω$$"
      ],
      "metadata": {
        "id": "aisnM5Ve6b-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "optimizer = optim.SGD(..., momentum = 0.9)"
      ],
      "metadata": {
        "id": "6VVsbp7E6u-Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "93cd6304-c02d-41b3-b41c-f1ae9f9464f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-49ef0d979039>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, momentum, dampening, weight_decay, nesterov, maximize, foreach, differentiable)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdampening\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nesterov momentum requires a momentum and zero dampening\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimizer got an empty parameter list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'ellipsis' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AdaGrad\n",
        "更新量が自動的に調整される特徴があり、学習が進むにつれて学習率が小さくなっていく。\n",
        "\n",
        "$$ h ← h+(\\frac{\\partial{E}}{\\partial{\\omega}})^2$$\n",
        "\n",
        "$$ \\omega ← \\omega -\\eta\\frac{1}{\\sqrt{h}}\\frac{\\partial{E}}{\\partial{\\omega}} $$"
      ],
      "metadata": {
        "id": "jWemQP621xaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "optimizer = optim.Adagrad(..."
      ],
      "metadata": {
        "id": "EgsD0Wcg1wYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSProp\n",
        "AdaGradの更新量の低下による学習の停滞問題を克服することが可能\n",
        "\n",
        "$$ h ← ρh+(1-ρ)(\\frac{\\partial{E}}{\\partial{\\omega}})^2$$\n",
        "\n",
        "$$ \\omega ← \\omega -\\eta\\frac{1}{\\sqrt{h}}\\frac{\\partial{E}}{\\partial{\\omega}} $$\\\n",
        "過去のhをある程度「忘却」し、更新量が低下したパラメータでも再び学習が進むようになる。"
      ],
      "metadata": {
        "id": "ms6Pw-fo3sQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "optimizer = optim.RMSProp(..."
      ],
      "metadata": {
        "id": "QTIi-mbk3i3H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adam(Adaptive moment estimation)\n",
        "様々な最適化アルゴリズムよりも高い性能を発揮することがある。\n",
        "$$ m_0 = v_0 =0 $$\n",
        "$$ m_t = \\beta_{1}m_{t-1} + (1-\\beta_{1})\\frac{\\partial{E}}{\\partial{\\omega}}$$\n",
        "$$ v_t = \\beta_{2}v_{t-1} + (1-\\beta_{2})\\frac{\\partial{E}}{\\partial{\\omega}} $$\n",
        "$$ \\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}$$\n",
        "$$ \\hat{v}_t  =\\frac{v_t}{1-\\beta_2^t} $$\n",
        "$$ \\omega \\leftarrow \\omega - \\eta \\frac{ \\hat{m}_t}{\\sqrt{\\hat{v}_t}+ϵ}$$\n",
        "\n",
        "$\\beta_1, \\beta_2, \\eta,  \\epsilon$　は定数部分で、$t$はパラメータの更新回数\n",
        "\n"
      ],
      "metadata": {
        "id": "35kBDwWe4Tsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "optimizer = optim.Adam(..."
      ],
      "metadata": {
        "id": "yxnVpm2N7y4N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}